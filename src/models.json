[
  {
    "id": "sonnet-4.5",
    "handle": "anthropic/claude-sonnet-4-5-20250929",
    "label": "Sonnet 4.5",
    "description": "The recommended default model",
    "isDefault": true,
    "isFeatured": true,
    "updateArgs": {
      "context_window": 180000,
      "max_output_tokens": 64000,
      "max_reasoning_tokens": 31999
    }
  },
  {
    "id": "sonnet-4.5-no-reasoning",
    "handle": "anthropic/claude-sonnet-4-5-20250929",
    "label": "Sonnet 4.5",
    "description": "Sonnet 4.5 with no reasoning (faster)",
    "updateArgs": {
      "enable_reasoner": false,
      "context_window": 180000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "opus",
    "handle": "anthropic/claude-opus-4-5-20251101",
    "label": "Opus 4.5",
    "description": "Anthropic's best model",
    "isFeatured": true,
    "updateArgs": {
      "context_window": 180000,
      "max_output_tokens": 64000,
      "max_reasoning_tokens": 31999
    }
  },
  {
    "id": "bedrock-opus",
    "handle": "bedrock/us.anthropic.claude-opus-4-5-20251101-v1:0",
    "label": "Bedrock Opus 4.5",
    "shortLabel": "Opus 4.5 BR",
    "description": "Anthropic's best model (via AWS Bedrock)",
    "isFeatured": true,
    "updateArgs": {
      "context_window": 180000,
      "max_output_tokens": 64000,
      "max_reasoning_tokens": 31999
    }
  },
  {
    "id": "haiku",
    "handle": "anthropic/claude-haiku-4-5-20251001",
    "label": "Haiku 4.5",
    "description": "Anthropic's fastest model",
    "isFeatured": true,
    "updateArgs": {
      "context_window": 180000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "gpt-5.2-codex-plus-pro-medium",
    "handle": "chatgpt-plus-pro/gpt-5.2-codex",
    "label": "GPT-5.2 Codex",
    "description": "GPT-5.2 Codex (med reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-codex-plus-pro-high",
    "handle": "chatgpt-plus-pro/gpt-5.2-codex",
    "label": "GPT-5.2 Codex",
    "description": "GPT-5.2 Codex (high reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-plus-pro-medium",
    "handle": "chatgpt-plus-pro/gpt-5.2",
    "label": "GPT-5.2",
    "description": "GPT-5.2 (med reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-plus-pro-high",
    "handle": "chatgpt-plus-pro/gpt-5.2",
    "label": "GPT-5.2",
    "description": "GPT-5.2 (high reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.3-codex-plus-pro-medium",
    "handle": "chatgpt-plus-pro/gpt-5.3-codex",
    "label": "GPT-5.3 Codex",
    "description": "GPT-5.3 Codex (med reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.3-codex-plus-pro-high",
    "handle": "chatgpt-plus-pro/gpt-5.3-codex",
    "label": "GPT-5.3 Codex",
    "description": "GPT-5.3 Codex (high reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.3-codex-plus-pro-xhigh",
    "handle": "chatgpt-plus-pro/gpt-5.3-codex",
    "label": "GPT-5.3 Codex",
    "description": "GPT-5.3 Codex (extra-high reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "xhigh",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-plus-pro-medium",
    "handle": "chatgpt-plus-pro/gpt-5.1-codex",
    "label": "GPT-5.1 Codex",
    "description": "GPT-5.1 Codex (med reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-plus-pro-high",
    "handle": "chatgpt-plus-pro/gpt-5.1-codex",
    "label": "GPT-5.1 Codex",
    "description": "GPT-5.1 Codex (high reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-max-plus-pro-medium",
    "handle": "chatgpt-plus-pro/gpt-5.1-codex-max",
    "label": "GPT-5.1 Codex Max",
    "description": "GPT-5.1 Codex Max (med reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-max-plus-pro-high",
    "handle": "chatgpt-plus-pro/gpt-5.1-codex-max",
    "label": "GPT-5.1 Codex Max",
    "description": "GPT-5.1 Codex Max (high reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-max-plus-pro-xhigh",
    "handle": "chatgpt-plus-pro/gpt-5.1-codex-max",
    "label": "GPT-5.1 Codex Max",
    "description": "GPT-5.1 Codex Max (extra-high reasoning) via ChatGPT Plus/Pro",
    "updateArgs": {
      "reasoning_effort": "xhigh",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5-codex",
    "handle": "openai/gpt-5-codex",
    "label": "GPT-5-Codex",
    "description": "GPT-5 variant (med reasoning) optimized for coding",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-none",
    "handle": "openai/gpt-5.2",
    "label": "GPT-5.2",
    "description": "Latest general-purpose GPT (no reasoning)",
    "updateArgs": {
      "reasoning_effort": "none",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-low",
    "handle": "openai/gpt-5.2",
    "label": "GPT-5.2",
    "description": "Latest general-purpose GPT (low reasoning)",
    "updateArgs": {
      "reasoning_effort": "low",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-medium",
    "handle": "openai/gpt-5.2",
    "label": "GPT-5.2",
    "description": "Latest general-purpose GPT (med reasoning)",
    "isFeatured": true,
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-high",
    "handle": "openai/gpt-5.2",
    "label": "GPT-5.2",
    "description": "Latest general-purpose GPT (high reasoning)",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-xhigh",
    "handle": "openai/gpt-5.2",
    "label": "GPT-5.2",
    "description": "Latest general-purpose GPT (max reasoning)",
    "updateArgs": {
      "reasoning_effort": "xhigh",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-codex-none",
    "handle": "openai/gpt-5.2-codex",
    "label": "GPT-5.2-Codex",
    "description": "GPT-5.2 variant (no reasoning) optimized for coding",
    "updateArgs": {
      "reasoning_effort": "none",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-codex-low",
    "handle": "openai/gpt-5.2-codex",
    "label": "GPT-5.2-Codex",
    "description": "GPT-5.2 variant (low reasoning) optimized for coding",
    "updateArgs": {
      "reasoning_effort": "low",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-codex-medium",
    "handle": "openai/gpt-5.2-codex",
    "label": "GPT-5.2-Codex",
    "description": "GPT-5.2 variant (med reasoning) optimized for coding",
    "isFeatured": true,
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-codex-high",
    "handle": "openai/gpt-5.2-codex",
    "label": "GPT-5.2-Codex",
    "description": "GPT-5.2 variant (high reasoning) optimized for coding",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.2-codex-xhigh",
    "handle": "openai/gpt-5.2-codex",
    "label": "GPT-5.2-Codex",
    "description": "GPT-5.2 variant (max reasoning) optimized for coding",
    "updateArgs": {
      "reasoning_effort": "xhigh",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.3-codex-low",
    "handle": "openai/gpt-5.3-codex",
    "label": "GPT-5.3-Codex",
    "description": "GPT-5.3 variant (low reasoning) optimized for coding",
    "updateArgs": {
      "reasoning_effort": "low",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.3-codex-medium",
    "handle": "openai/gpt-5.3-codex",
    "label": "GPT-5.3-Codex",
    "description": "GPT-5.3 variant (med reasoning) optimized for coding",
    "isFeatured": true,
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.3-codex-high",
    "handle": "openai/gpt-5.3-codex",
    "label": "GPT-5.3-Codex",
    "description": "GPT-5.3 variant (high reasoning) optimized for coding",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.3-codex-xhigh",
    "handle": "openai/gpt-5.3-codex",
    "label": "GPT-5.3-Codex",
    "description": "GPT-5.3 variant (max reasoning) optimized for coding",
    "updateArgs": {
      "reasoning_effort": "xhigh",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-none",
    "handle": "openai/gpt-5.1",
    "label": "GPT-5.1",
    "description": "Legacy GPT-5.1 (no reasoning)",
    "updateArgs": {
      "reasoning_effort": "none",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-low",
    "handle": "openai/gpt-5.1",
    "label": "GPT-5.1",
    "description": "Legacy GPT-5.1 (low reasoning)",
    "updateArgs": {
      "reasoning_effort": "low",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-medium",
    "handle": "openai/gpt-5.1",
    "label": "GPT-5.1",
    "description": "Legacy GPT-5.1 (med reasoning)",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-high",
    "handle": "openai/gpt-5.1",
    "label": "GPT-5.1",
    "description": "Legacy GPT-5.1 (high reasoning)",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-none",
    "handle": "openai/gpt-5.1-codex",
    "label": "GPT-5.1-Codex",
    "description": "GPT-5.1 variant (no reasoning) optimized for coding",
    "updateArgs": {
      "reasoning_effort": "none",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-medium",
    "handle": "openai/gpt-5.1-codex",
    "label": "GPT-5.1-Codex",
    "description": "GPT-5.1 variant (med reasoning) optimized for coding",
    "isFeatured": true,
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-high",
    "handle": "openai/gpt-5.1-codex",
    "label": "GPT-5.1-Codex",
    "description": "GPT-5.1 variant (max reasoning) optimized for coding",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-max-medium",
    "handle": "openai/gpt-5.1-codex-max",
    "label": "GPT-5.1-Codex-Max",
    "description": "GPT-5.1-Codex 'Max' variant (med reasoning)",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-max-high",
    "handle": "openai/gpt-5.1-codex-max",
    "label": "GPT-5.1-Codex-Max",
    "description": "GPT-5.1-Codex 'Max' variant (high reasoning)",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5.1-codex-max-x-high",
    "handle": "openai/gpt-5.1-codex-max",
    "label": "GPT-5.1-Codex-Max",
    "description": "GPT-5.1-Codex 'Max' variant (extra-high reasoning)",
    "updateArgs": {
      "reasoning_effort": "xhigh",
      "verbosity": "medium",
      "context_window": 272000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "gpt-5-minimal",
    "handle": "openai/gpt-5",
    "label": "GPT-5",
    "description": "Legacy GPT-5 (minimal reasoning)",
    "updateArgs": {
      "reasoning_effort": "minimal",
      "verbosity": "medium",
      "context_window": 272000
    }
  },
  {
    "id": "gpt-5-low",
    "handle": "openai/gpt-5",
    "label": "GPT-5",
    "description": "Legacy GPT-5 (low reasoning)",
    "updateArgs": {
      "reasoning_effort": "low",
      "verbosity": "medium",
      "context_window": 272000
    }
  },
  {
    "id": "gpt-5-medium",
    "handle": "openai/gpt-5",
    "label": "GPT-5",
    "description": "Legacy GPT-5 (med reasoning)",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000
    }
  },
  {
    "id": "gpt-5-high",
    "handle": "openai/gpt-5",
    "label": "GPT-5",
    "description": "Legacy GPT-5 (high reasoning)",
    "updateArgs": {
      "reasoning_effort": "high",
      "verbosity": "medium",
      "context_window": 272000
    }
  },
  {
    "id": "gpt-5-mini-medium",
    "handle": "openai/gpt-5-mini-2025-08-07",
    "label": "GPT-5-Mini",
    "description": "GPT-5-Mini (medium reasoning)",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000
    }
  },
  {
    "id": "gpt-5-nano-medium",
    "handle": "openai/gpt-5-nano-2025-08-07",
    "label": "GPT-5-Nano",
    "description": "GPT-5-Nano (medium reasoning)",
    "updateArgs": {
      "reasoning_effort": "medium",
      "verbosity": "medium",
      "context_window": 272000
    }
  },
  {
    "id": "glm-4.6",
    "handle": "zai/glm-4.6",
    "label": "GLM-4.6",
    "description": "Zai's legacy model",
    "updateArgs": {
      "context_window": 200000
    }
  },
  {
    "id": "glm-5",
    "handle": "zai/glm-5",
    "label": "GLM-5",
    "description": "zAI's latest coding model",
    "isFeatured": true,
    "free": true,
    "updateArgs": {
      "context_window": 200000
    }
  },
  {
    "id": "zai-glm-5",
    "handle": "cliproxy/zai-glm-5",
    "label": "GLM-5 (CLIProxy)",
    "description": "Zai GLM-5 via CLIProxyAPI passthru",
    "isFeatured": true,
    "updateArgs": {
      "context_window": 204800,
      "max_output_tokens": 131072
    }
  },
  {
    "id": "minimax-m2.1",
    "handle": "minimax/MiniMax-M2.1",
    "label": "MiniMax 2.1",
    "description": "MiniMax's latest coding model",
    "isFeatured": true,
    "free": true,
    "updateArgs": {
      "context_window": 180000
    }
  },
  {
    "id": "minimax-m2.1-cliproxy",
    "handle": "cliproxy/minimax-m2.1",
    "label": "Minimax M2.1 (CLIProxy)",
    "description": "Minimax M2.1 via CLIProxyAPI passthru",
    "updateArgs": {
      "context_window": 190000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "minimax-m2",
    "handle": "openrouter/minimax/minimax-m2",
    "label": "Minimax M2",
    "description": "Minimax's latest model",
    "updateArgs": {
      "context_window": 196000
    }
  },
  {
    "id": "kimi-k2",
    "handle": "openrouter/moonshotai/kimi-k2-0905",
    "label": "Kimi K2",
    "description": "Kimi's K2 model",
    "updateArgs": {
      "context_window": 262144
    }
  },
  {
    "id": "kimi-k2-thinking",
    "handle": "openrouter/moonshotai/kimi-k2-thinking",
    "label": "Kimi K2 Thinking",
    "description": "Kimi's K2 model with advanced thinking capabilities",
    "updateArgs": {
      "context_window": 256000,
      "max_output_tokens": 16000,
      "temperature": 1.0
    }
  },
  {
    "id": "kimi-k2.5",
    "handle": "openrouter/moonshotai/kimi-k2.5",
    "label": "Kimi K2.5",
    "description": "Kimi's latest coding model",
    "isFeatured": true,
    "updateArgs": {
      "context_window": 262144
    }
  },
  {
    "id": "deepseek-chat-v3.1",
    "handle": "openrouter/deepseek/deepseek-chat-v3.1",
    "label": "DeepSeek Chat V3.1",
    "description": "DeepSeek V3.1 model",
    "updateArgs": {
      "context_window": 128000
    }
  },
  {
    "id": "gemini-3",
    "handle": "google_ai/gemini-3-pro-preview",
    "label": "Gemini 3 Pro",
    "description": "Google's smartest model",
    "isFeatured": true,
    "updateArgs": {
      "context_window": 1000000,
      "temperature": 1.0,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "gemini-3-flash",
    "handle": "google_ai/gemini-3-flash-preview",
    "label": "Gemini 3 Flash",
    "description": "Google's fastest Gemini 3 model",
    "isFeatured": true,
    "updateArgs": {
      "context_window": 1048576,
      "temperature": 1.0,
      "max_output_tokens": 65536
    }
  },
  {
    "id": "gemini-flash",
    "handle": "google_ai/gemini-2.5-flash",
    "label": "Gemini 2.5 Flash",
    "description": "Google's fastest model",
    "updateArgs": {
      "context_window": 1048576,
      "max_output_tokens": 65536
    }
  },
  {
    "id": "gemini-pro",
    "handle": "google_ai/gemini-2.5-pro",
    "label": "Gemini 2.5 Pro",
    "description": "Google's last generation flagship model",
    "updateArgs": {
      "context_window": 1048576,
      "max_output_tokens": 65536
    }
  },
  {
    "id": "gpt-4.1",
    "handle": "openai/gpt-4.1",
    "label": "GPT-4.1",
    "description": "OpenAI's most recent non-reasoner model",
    "updateArgs": {
      "context_window": 1047576
    }
  },
  {
    "id": "gpt-4.1-mini",
    "handle": "openai/gpt-4.1-mini-2025-04-14",
    "label": "GPT-4.1-Mini",
    "description": "OpenAI's most recent non-reasoner model (mini version)",
    "updateArgs": {
      "context_window": 1047576
    }
  },
  {
    "id": "gpt-4.1-nano",
    "handle": "openai/gpt-4.1-nano-2025-04-14",
    "label": "GPT-4.1-Nano",
    "description": "OpenAI's most recent non-reasoner model (nano version)",
    "updateArgs": {
      "context_window": 1047576
    }
  },
  {
    "id": "o4-mini",
    "handle": "openai/o4-mini",
    "label": "o4-mini",
    "description": "OpenAI's latest o-series reasoning model",
    "updateArgs": {
      "context_window": 180000
    }
  },
  {
    "id": "gemini-3-vertex",
    "handle": "google_vertex/gemini-3-pro-preview",
    "label": "Gemini 3 Pro",
    "description": "Google's smartest Gemini 3 Pro model (via Vertex AI)",
    "updateArgs": {
      "context_window": 1048576,
      "temperature": 1.0,
      "max_output_tokens": 65536
    }
  },
  {
    "id": "cliproxy-gpt-5.2-xhigh",
    "handle": "cliproxy/gpt-5.2-xhigh",
    "label": "GPT-5.2 XHigh (CLIProxy)",
    "description": "GPT-5.2 extra-high reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.2-high",
    "handle": "cliproxy/gpt-5.2-high",
    "label": "GPT-5.2 High (CLIProxy)",
    "description": "GPT-5.2 high reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.2-medium",
    "handle": "cliproxy/gpt-5.2-medium",
    "label": "GPT-5.2 Medium (CLIProxy, default)",
    "description": "GPT-5.2 via CLIProxyAPI (recommended default)",
    "isDefault": true,
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.2-low",
    "handle": "cliproxy/gpt-5.2-low",
    "label": "GPT-5.2 Low (CLIProxy)",
    "description": "GPT-5.2 low reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.2-minimal",
    "handle": "cliproxy/gpt-5.2-minimal",
    "label": "GPT-5.2 Minimal (CLIProxy)",
    "description": "GPT-5.2 minimal reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.2-codex-xhigh",
    "handle": "cliproxy/gpt-5.2-codex-xhigh",
    "label": "GPT-5.2 Codex XHigh (CLIProxy)",
    "description": "GPT-5.2 Codex extra-high reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.2-codex-high",
    "handle": "cliproxy/gpt-5.2-codex-high",
    "label": "GPT-5.2 Codex High (CLIProxy)",
    "description": "GPT-5.2 Codex high reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.2-codex-medium",
    "handle": "cliproxy/gpt-5.2-codex-medium",
    "label": "GPT-5.2 Codex Medium (CLIProxy)",
    "description": "GPT-5.2 Codex medium reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.2-codex-low",
    "handle": "cliproxy/gpt-5.2-codex-low",
    "label": "GPT-5.2 Codex Low (CLIProxy)",
    "description": "GPT-5.2 Codex low reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.2-codex",
    "handle": "cliproxy/gpt-5.2-codex",
    "label": "GPT-5.2 Codex (CLIProxy)",
    "description": "GPT-5.2 Codex via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.3-codex-xhigh",
    "handle": "cliproxy/gpt-5.3-codex-xhigh",
    "label": "GPT-5.3 Codex XHigh (CLIProxy)",
    "description": "GPT-5.3 Codex extra-high reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.3-codex-high",
    "handle": "cliproxy/gpt-5.3-codex-high",
    "label": "GPT-5.3 Codex High (CLIProxy)",
    "description": "GPT-5.3 Codex high reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.3-codex-medium",
    "handle": "cliproxy/gpt-5.3-codex-medium",
    "label": "GPT-5.3 Codex Medium (CLIProxy)",
    "description": "GPT-5.3 Codex medium reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.3-codex-low",
    "handle": "cliproxy/gpt-5.3-codex-low",
    "label": "GPT-5.3 Codex Low (CLIProxy)",
    "description": "GPT-5.3 Codex low reasoning via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.3-codex",
    "handle": "cliproxy/gpt-5.3-codex",
    "label": "GPT-5.3 Codex (CLIProxy)",
    "description": "GPT-5.3 Codex via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.1-codex-max-xhigh",
    "handle": "cliproxy/gpt-5.1-codex-max-xhigh",
    "label": "GPT-5.1 Codex Max XHigh (CLIProxy)",
    "description": "GPT-5.1 Codex Max extra-high via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.1-codex-max-high",
    "handle": "cliproxy/gpt-5.1-codex-max-high",
    "label": "GPT-5.1 Codex Max High (CLIProxy)",
    "description": "GPT-5.1 Codex Max high via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.1-codex-max-medium",
    "handle": "cliproxy/gpt-5.1-codex-max-medium",
    "label": "GPT-5.1 Codex Max Medium (CLIProxy)",
    "description": "GPT-5.1 Codex Max medium via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.1-codex-max-low",
    "handle": "cliproxy/gpt-5.1-codex-max-low",
    "label": "GPT-5.1 Codex Max Low (CLIProxy)",
    "description": "GPT-5.1 Codex Max low via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.1-codex-high",
    "handle": "cliproxy/gpt-5.1-codex-high",
    "label": "GPT-5.1 Codex High (CLIProxy)",
    "description": "GPT-5.1 Codex high via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.1-codex-medium",
    "handle": "cliproxy/gpt-5.1-codex-medium",
    "label": "GPT-5.1 Codex Medium (CLIProxy)",
    "description": "GPT-5.1 Codex medium via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.1-codex-low",
    "handle": "cliproxy/gpt-5.1-codex-low",
    "label": "GPT-5.1 Codex Low (CLIProxy)",
    "description": "GPT-5.1 Codex low via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.1-codex-mini-high",
    "handle": "cliproxy/gpt-5.1-codex-mini-high",
    "label": "GPT-5.1 Codex Mini High (CLIProxy)",
    "description": "GPT-5.1 Codex Mini high via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5.1-codex-mini-medium",
    "handle": "cliproxy/gpt-5.1-codex-mini-medium",
    "label": "GPT-5.1 Codex Mini Medium (CLIProxy)",
    "description": "GPT-5.1 Codex Mini medium via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gpt-5-codex",
    "handle": "cliproxy/gpt-5-codex",
    "label": "GPT-5 Codex (CLIProxy)",
    "description": "GPT-5 Codex via CLIProxyAPI",
    "updateArgs": {
      "context_window": 400000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-gemini-3-pro",
    "handle": "cliproxy/gemini-3-pro-preview",
    "label": "Gemini 3 Pro (CLIProxy)",
    "description": "Gemini 3 Pro via CLIProxyAPI",
    "updateArgs": {
      "context_window": 1000000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-gemini-3-flash",
    "handle": "cliproxy/gemini-3-flash-preview",
    "label": "Gemini 3 Flash (CLIProxy)",
    "description": "Gemini 3 Flash via CLIProxyAPI",
    "updateArgs": {
      "context_window": 1048576,
      "max_output_tokens": 65536
    }
  },
  {
    "id": "cliproxy-gemini-2.5-pro",
    "handle": "cliproxy/gemini-2.5-pro",
    "label": "Gemini 2.5 Pro (CLIProxy)",
    "description": "Gemini 2.5 Pro via CLIProxyAPI",
    "updateArgs": {
      "context_window": 1048576,
      "max_output_tokens": 65536
    }
  },
  {
    "id": "cliproxy-gemini-2.5-flash",
    "handle": "cliproxy/gemini-2.5-flash",
    "label": "Gemini 2.5 Flash (CLIProxy)",
    "description": "Gemini 2.5 Flash via CLIProxyAPI",
    "updateArgs": {
      "context_window": 1048576,
      "max_output_tokens": 65536
    }
  },
  {
    "id": "cliproxy-qwen3-coder-plus",
    "handle": "cliproxy/qwen3-coder-plus",
    "label": "Qwen3 Coder Plus (CLIProxy)",
    "description": "Qwen3 Coder Plus via CLIProxyAPI",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-qwen3-coder-flash",
    "handle": "cliproxy/qwen3-coder-flash",
    "label": "Qwen3 Coder Flash (CLIProxy)",
    "description": "Qwen3 Coder Flash via CLIProxyAPI",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 32768
    }
  },
  {
    "id": "cliproxy-copilot-claude-sonnet-4.5",
    "handle": "cliproxy/copilot-claude-sonnet-4.5",
    "label": "Claude Sonnet 4.5 (CLIProxy/Copilot)",
    "description": "Claude Sonnet 4.5 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 16000
    }
  },
  {
    "id": "cliproxy-copilot-claude-opus-4.5",
    "handle": "cliproxy/copilot-claude-opus-4.5",
    "label": "Claude Opus 4.5 (CLIProxy/Copilot)",
    "description": "Claude Opus 4.5 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 16000
    }
  },
  {
    "id": "cliproxy-copilot-claude-opus-4.6",
    "handle": "cliproxy/copilot-claude-opus-4.6",
    "label": "Claude Opus 4.6 (CLIProxy/Copilot)",
    "description": "Claude Opus 4.6 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 200000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-claude-opus-4.6-fast",
    "handle": "cliproxy/copilot-claude-opus-4.6-fast",
    "label": "Claude Opus 4.6 Fast (Preview) (CLIProxy/Copilot)",
    "description": "Claude Opus 4.6 fast mode preview via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 200000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "opus-4.6",
    "handle": "cliproxy/copilot-claude-opus-4.6",
    "label": "Opus 4.6",
    "description": "Claude Opus 4.6 via CLIProxyAPI Copilot (short alias)",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "opus-4.6-fast",
    "handle": "cliproxy/copilot-claude-opus-4.6-fast",
    "label": "Opus 4.6 Fast (Preview)",
    "description": "Claude Opus 4.6 fast mode preview via CLIProxyAPI Copilot (short alias)",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-claude-haiku-4.5",
    "handle": "cliproxy/copilot-claude-haiku-4.5",
    "label": "Claude Haiku 4.5 (CLIProxy/Copilot)",
    "description": "Claude Haiku 4.5 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 16000
    }
  },
  {
    "id": "cliproxy-copilot-claude-opus-4.1",
    "handle": "cliproxy/copilot-claude-opus-4.1",
    "label": "Claude Opus 4.1 (CLIProxy/Copilot)",
    "description": "Claude Opus 4.1 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 80000,
      "max_output_tokens": 16000
    }
  },
  {
    "id": "cliproxy-copilot-claude-sonnet-4",
    "handle": "cliproxy/copilot-claude-sonnet-4",
    "label": "Claude Sonnet 4 (CLIProxy/Copilot)",
    "description": "Claude Sonnet 4 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 16000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex",
    "handle": "cliproxy/copilot-gpt-5.1-codex",
    "label": "GPT-5.1 Codex (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-low",
    "handle": "cliproxy/copilot-gpt-5.1-codex-low",
    "label": "GPT-5.1 Codex Low (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex low reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-medium",
    "handle": "cliproxy/copilot-gpt-5.1-codex-medium",
    "label": "GPT-5.1 Codex Medium (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex medium reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-high",
    "handle": "cliproxy/copilot-gpt-5.1-codex-high",
    "label": "GPT-5.1 Codex High (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-max",
    "handle": "cliproxy/copilot-gpt-5.1-codex-max",
    "label": "GPT-5.1 Codex Max (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex Max via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-max-low",
    "handle": "cliproxy/copilot-gpt-5.1-codex-max-low",
    "label": "GPT-5.1 Codex Max Low (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex Max low reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-max-medium",
    "handle": "cliproxy/copilot-gpt-5.1-codex-max-medium",
    "label": "GPT-5.1 Codex Max Medium (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex Max medium reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-max-high",
    "handle": "cliproxy/copilot-gpt-5.1-codex-max-high",
    "label": "GPT-5.1 Codex Max High (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex Max high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-max-xhigh",
    "handle": "cliproxy/copilot-gpt-5.1-codex-max-xhigh",
    "label": "GPT-5.1 Codex Max XHigh (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex Max extra-high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-mini",
    "handle": "cliproxy/copilot-gpt-5.1-codex-mini",
    "label": "GPT-5.1 Codex Mini (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex Mini via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 100000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-mini-low",
    "handle": "cliproxy/copilot-gpt-5.1-codex-mini-low",
    "label": "GPT-5.1 Codex Mini Low (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex Mini low reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 100000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-mini-medium",
    "handle": "cliproxy/copilot-gpt-5.1-codex-mini-medium",
    "label": "GPT-5.1 Codex Mini Medium (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex Mini medium reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 100000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-codex-mini-high",
    "handle": "cliproxy/copilot-gpt-5.1-codex-mini-high",
    "label": "GPT-5.1 Codex Mini High (CLIProxy/Copilot)",
    "description": "GPT-5.1 Codex Mini high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 100000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1",
    "handle": "cliproxy/copilot-gpt-5.1",
    "label": "GPT-5.1 (CLIProxy/Copilot)",
    "description": "GPT-5.1 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-low",
    "handle": "cliproxy/copilot-gpt-5.1-low",
    "label": "GPT-5.1 Low (CLIProxy/Copilot)",
    "description": "GPT-5.1 low reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-medium",
    "handle": "cliproxy/copilot-gpt-5.1-medium",
    "label": "GPT-5.1 Medium (CLIProxy/Copilot)",
    "description": "GPT-5.1 medium reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.1-high",
    "handle": "cliproxy/copilot-gpt-5.1-high",
    "label": "GPT-5.1 High (CLIProxy/Copilot)",
    "description": "GPT-5.1 high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5",
    "handle": "cliproxy/copilot-gpt-5",
    "label": "GPT-5 (CLIProxy/Copilot)",
    "description": "GPT-5 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5-low",
    "handle": "cliproxy/copilot-gpt-5-low",
    "label": "GPT-5 Low (CLIProxy/Copilot)",
    "description": "GPT-5 low reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5-medium",
    "handle": "cliproxy/copilot-gpt-5-medium",
    "label": "GPT-5 Medium (CLIProxy/Copilot)",
    "description": "GPT-5 medium reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5-high",
    "handle": "cliproxy/copilot-gpt-5-high",
    "label": "GPT-5 High (CLIProxy/Copilot)",
    "description": "GPT-5 high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5-mini",
    "handle": "cliproxy/copilot-gpt-5-mini",
    "label": "GPT-5 Mini (CLIProxy/Copilot)",
    "description": "GPT-5 Mini via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5-mini-low",
    "handle": "cliproxy/copilot-gpt-5-mini-low",
    "label": "GPT-5 Mini Low (CLIProxy/Copilot)",
    "description": "GPT-5 Mini low reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5-mini-medium",
    "handle": "cliproxy/copilot-gpt-5-mini-medium",
    "label": "GPT-5 Mini Medium (CLIProxy/Copilot)",
    "description": "GPT-5 Mini medium reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5-mini-high",
    "handle": "cliproxy/copilot-gpt-5-mini-high",
    "label": "GPT-5 Mini High (CLIProxy/Copilot)",
    "description": "GPT-5 Mini high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5-codex",
    "handle": "cliproxy/copilot-gpt-5-codex",
    "label": "GPT-5 Codex (CLIProxy/Copilot)",
    "description": "GPT-5 Codex via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.2",
    "handle": "cliproxy/copilot-gpt-5.2",
    "label": "GPT-5.2 (CLIProxy/Copilot)",
    "description": "GPT-5.2 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.2-low",
    "handle": "cliproxy/copilot-gpt-5.2-low",
    "label": "GPT-5.2 Low (CLIProxy/Copilot)",
    "description": "GPT-5.2 low reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.2-medium",
    "handle": "cliproxy/copilot-gpt-5.2-medium",
    "label": "GPT-5.2 Medium (CLIProxy/Copilot)",
    "description": "GPT-5.2 medium reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.2-high",
    "handle": "cliproxy/copilot-gpt-5.2-high",
    "label": "GPT-5.2 High (CLIProxy/Copilot)",
    "description": "GPT-5.2 high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.2-xhigh",
    "handle": "cliproxy/copilot-gpt-5.2-xhigh",
    "label": "GPT-5.2 XHigh (CLIProxy/Copilot)",
    "description": "GPT-5.2 extra-high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.2-codex",
    "handle": "cliproxy/copilot-gpt-5.2-codex",
    "label": "GPT-5.2 Codex (CLIProxy/Copilot)",
    "description": "GPT-5.2 Codex via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.2-codex-low",
    "handle": "cliproxy/copilot-gpt-5.2-codex-low",
    "label": "GPT-5.2 Codex Low (CLIProxy/Copilot)",
    "description": "GPT-5.2 Codex low reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.2-codex-medium",
    "handle": "cliproxy/copilot-gpt-5.2-codex-medium",
    "label": "GPT-5.2 Codex Medium (CLIProxy/Copilot)",
    "description": "GPT-5.2 Codex medium reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.2-codex-high",
    "handle": "cliproxy/copilot-gpt-5.2-codex-high",
    "label": "GPT-5.2 Codex High (CLIProxy/Copilot)",
    "description": "GPT-5.2 Codex high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.2-codex-xhigh",
    "handle": "cliproxy/copilot-gpt-5.2-codex-xhigh",
    "label": "GPT-5.2 Codex XHigh (CLIProxy/Copilot)",
    "description": "GPT-5.2 Codex extra-high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.3-codex",
    "handle": "cliproxy/copilot-gpt-5.3-codex",
    "label": "GPT-5.3 Codex (CLIProxy/Copilot)",
    "description": "GPT-5.3 Codex via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.3-codex-low",
    "handle": "cliproxy/copilot-gpt-5.3-codex-low",
    "label": "GPT-5.3 Codex Low (CLIProxy/Copilot)",
    "description": "GPT-5.3 Codex low reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.3-codex-medium",
    "handle": "cliproxy/copilot-gpt-5.3-codex-medium",
    "label": "GPT-5.3 Codex Medium (CLIProxy/Copilot)",
    "description": "GPT-5.3 Codex medium reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.3-codex-high",
    "handle": "cliproxy/copilot-gpt-5.3-codex-high",
    "label": "GPT-5.3 Codex High (CLIProxy/Copilot)",
    "description": "GPT-5.3 Codex high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-5.3-codex-xhigh",
    "handle": "cliproxy/copilot-gpt-5.3-codex-xhigh",
    "label": "GPT-5.3 Codex XHigh (CLIProxy/Copilot)",
    "description": "GPT-5.3 Codex extra-high reasoning via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 128000
    }
  },
  {
    "id": "cliproxy-copilot-gpt-4.1",
    "handle": "cliproxy/copilot-gpt-4.1",
    "label": "GPT-4.1 (CLIProxy/Copilot)",
    "description": "GPT-4.1 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 16384
    }
  },
  {
    "id": "cliproxy-copilot-gpt-4o",
    "handle": "cliproxy/copilot-gpt-4o",
    "label": "GPT-4o (CLIProxy/Copilot)",
    "description": "GPT-4o via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 64000,
      "max_output_tokens": 16384
    }
  },
  {
    "id": "cliproxy-copilot-gemini-3-pro",
    "handle": "cliproxy/copilot-gemini-3-pro-preview",
    "label": "Gemini 3 Pro (CLIProxy/Copilot)",
    "description": "Gemini 3 Pro via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-gemini-3-flash",
    "handle": "cliproxy/copilot-gemini-3-flash-preview",
    "label": "Gemini 3 Flash (CLIProxy/Copilot)",
    "description": "Gemini 3 Flash via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-gemini-2.5-pro",
    "handle": "cliproxy/copilot-gemini-2.5-pro",
    "label": "Gemini 2.5 Pro (CLIProxy/Copilot)",
    "description": "Gemini 2.5 Pro via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-gemini-2.5-flash",
    "handle": "cliproxy/copilot-gemini-2.5-flash",
    "label": "Gemini 2.5 Flash (CLIProxy/Copilot)",
    "description": "Gemini 2.5 Flash via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 1000000,
      "max_output_tokens": 8192
    }
  },
  {
    "id": "cliproxy-gemini-claude-sonnet-4.5-thinking",
    "handle": "cliproxy/gemini-claude-sonnet-4-5-thinking",
    "label": "Claude Sonnet 4.5 Thinking (CLIProxy/Gemini)",
    "description": "Claude Sonnet 4.5 via Gemini provider through CLIProxyAPI",
    "updateArgs": {
      "context_window": 200000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-gemini-claude-opus-4.5-thinking",
    "handle": "cliproxy/gemini-claude-opus-4-5-thinking",
    "label": "Claude Opus 4.5 Thinking (CLIProxy/Gemini)",
    "description": "Claude Opus 4.5 via Gemini provider through CLIProxyAPI",
    "updateArgs": {
      "context_window": 200000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-grok-code-fast-1",
    "handle": "cliproxy/copilot-grok-code-fast-1",
    "label": "Grok Code Fast 1 (CLIProxy/Copilot)",
    "description": "Grok Code Fast 1 via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 64000
    }
  },
  {
    "id": "cliproxy-copilot-raptor-mini",
    "handle": "cliproxy/copilot-raptor-mini",
    "label": "Raptor Mini (CLIProxy/Copilot)",
    "description": "Raptor Mini via CLIProxyAPI Copilot",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 32768
    }
  },
  {
    "id": "cliproxy-grok-4.1-thinking",
    "handle": "cliproxy/grok-4.1-thinking",
    "label": "Grok 4.1 Thinking (CLIProxy)",
    "description": "Grok 4.1 with thinking via CLIProxyAPI",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 32768
    }
  },
  {
    "id": "cliproxy-kimi-k2-thinking",
    "handle": "cliproxy/kimi-k2-thinking",
    "label": "Kimi K2 Thinking (CLIProxy)",
    "description": "Kimi K2 with thinking via CLIProxyAPI",
    "updateArgs": {
      "context_window": 256000,
      "max_output_tokens": 16000
    }
  },
  {
    "id": "chutes-kimi-k2-thinking",
    "handle": "cliproxy/chutes-moonshotai/Kimi-K2-Thinking",
    "label": "Kimi K2 Thinking (Chutes TEE)",
    "description": "Kimi K2 Thinking via Chutes confidential compute (TEE)",
    "updateArgs": {
      "context_window": 262144,
      "max_output_tokens": 65535
    }
  },
  {
    "id": "cliproxy-kimi-k2.5",
    "handle": "cliproxy/moonshotai/Kimi-K2.5",
    "label": "Kimi K2.5 (CLIProxy)",
    "description": "Kimi K2.5 via CLIProxyAPI",
    "updateArgs": {
      "context_window": 262144,
      "max_output_tokens": 65535
    }
  },
  {
    "id": "chutes-kimi-k2.5",
    "handle": "cliproxy/chutes-moonshotai/Kimi-K2.5",
    "label": "Kimi K2.5 (Chutes)",
    "description": "Kimi K2.5 via Chutes confidential compute",
    "updateArgs": {
      "context_window": 262144,
      "max_output_tokens": 65535
    }
  },
  {
    "id": "chutes-kimi-k2-instruct",
    "handle": "cliproxy/chutes-moonshotai/Kimi-K2-Instruct-0905",
    "label": "Kimi K2 Instruct (Chutes)",
    "description": "Kimi K2 Instruct via Chutes",
    "updateArgs": {
      "context_window": 262144,
      "max_output_tokens": 262144
    }
  },
  {
    "id": "chutes-deepseek-v3.2",
    "handle": "cliproxy/chutes-deepseek-ai/DeepSeek-V3.2",
    "label": "DeepSeek V3.2 (Chutes)",
    "description": "DeepSeek V3.2 via Chutes",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 32000
    }
  },
  {
    "id": "chutes-deepseek-r1",
    "handle": "cliproxy/chutes-deepseek-ai/DeepSeek-R1",
    "label": "DeepSeek R1 (Chutes)",
    "description": "DeepSeek R1 reasoning model via Chutes",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 32000
    }
  },
  {
    "id": "chutes-qwen3-coder",
    "handle": "cliproxy/chutes-Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
    "label": "Qwen3 Coder 480B (Chutes)",
    "description": "Qwen3 Coder 480B via Chutes",
    "updateArgs": {
      "context_window": 131072,
      "max_output_tokens": 32000
    }
  },
  {
    "id": "chutes-glm-5",
    "handle": "cliproxy/chutes-zai-org/GLM-5-TEE",
    "label": "GLM-5 (Chutes)",
    "description": "GLM-5 via Chutes",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 32000
    }
  },
  {
    "id": "chutes-glm-4.7-flash",
    "handle": "cliproxy/chutes-zai-org/GLM-4.7-Flash",
    "label": "GLM-4.7 Flash (Chutes)",
    "description": "GLM-4.7 Flash via Chutes",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 32000
    }
  },
  {
    "id": "chutes-minimax-m2",
    "handle": "cliproxy/chutes-MiniMaxAI/MiniMax-M2.1",
    "label": "MiniMax M2.1 (Chutes)",
    "description": "MiniMax M2.1 via Chutes",
    "updateArgs": {
      "context_window": 128000,
      "max_output_tokens": 32000
    }
  }
]
